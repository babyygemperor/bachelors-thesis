\chapter{Results}\label{chapter:results}

The outcomes of the annotation process using GPT and LLMs were compelling. GPT annotations exhibited superior quality and precision in explaining the identifiers. This chapter presents a comprehensive analysis of the results, including CoNLL scores, semantic accuracy scores, annotation costs, annotation duration, the extent of paper annotation by the LLMs, and the variance in scores due to the stochastic nature of LLMs.

\section{GPT}
We first scrutinised the annotations generated by GPT. For this analysis, we used all 40 papers from \cite{asakura2022building} as the ground truth and then employed GPT to generate dictionaries and annotations for all these papers.

\subsection{CoNLL Score}
The CoNLL score is a metric used to quantify the quality of coreference clusters. We obtained the weighted average of the CoNLL score across all the papers, with the weighting factored against the total number of annotations in the paper. Higher scores indicate better performance. As shown in Table \ref{tab:conll-score}, GPT-4 outperformed the other models, while GPT-3.5 exhibited the lowest score.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    gpt (average) & 79.31 & 76.89 \\
    gpt-3.5-turbo & 78.51 & 75.93 \\
    gpt-3.5-turbo-16k & 79.28 & 76.67 \\
    gpt-4 & 80.15 & 78.08\\
    \hline
  \end{tabular}
  \caption[CoNLL Scores]{Weighted average of CoNLL Scores.}
  \label{tab:conll-score}
\end{table}

\subsection{Coverage of Annotation}
Another critical aspect of this research was to examine the extent of the paper that the LLMs could successfully annotate. We represent this coverage in Table \ref{tab:anno-percentage}. GPT-4 demonstrated the highest coverage, while GPT-3.5 had the least.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lr}
    \hline
    Model & Coverage \\
    \hline
    gpt (average) & 90.57\% \\
    gpt-3.5-turbo & 90.64\% \\
    gpt-3.5-turbo-16k & 88.21\% \\
    gpt-4 & 92.87\% \\
    \hline
  \end{tabular}
  \caption[Total coverage]{coverage of the annotation of the papers achieved by GPT}
  \label{tab:anno-percentage}
\end{table}

\subsection{Semantic Accuracy}
Semantic accuracy provides a measure of the correctness of the annotations. We weighted it against the coverage to represent the total. Higher scores are preferable. As shown in Table \ref{tab:semantic-accuracy}, GPT-4 once again outperformed all the models by a significant margin.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    gpt (average) & 88.19\% & 79.87\% \\
    gpt-3.5-turbo & 84.69\% & 76.76\% \\
    gpt-3.5-turbo-16k & 84.17\% & 74.25\% \\
    gpt-4 & 95.70\% & 88.88\% \\
    \hline
  \end{tabular}
  \caption[Semantic Accuracy]{Weighted average of semantic accuracy}
  \label{tab:semantic-accuracy}
\end{table}

\subsection{Variance of Data}
Given the inherent stochastic nature of these models, some variation can be expected across multiple iterations. To account for this, we conducted multiple runs of the experiment on the reference paper ArXiV ID: 2107.10832 \citep{singleton2021logic} and tabulated the resultant variance. These outcomes are displayed in Table \ref{tab:variance}.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrrrr}
    \hline
    Model & Best & Worst & Mean & Median & Std. Deviation \\
    \hline
    gpt (average) & 86.75 & 83.21 & 84.58 & 84.18 & 1.55 \\
    gpt-3.5-turbo & 82.83 & 80.00 & 81.28 & 81.15 & 1.17 \\
    gpt-3.5-turbo-16k & 88.38 & 83.47 & 85.29 & 84.65 & 2.16 \\
    gpt-4 & 89.05 & 86.16 & 87.18 & 86.75 & 1.32 \\
    \hline
  \end{tabular}
  \caption[Statistics of variance]{Different CoNLL scores across four different runs}
  \label{tab:variance}
\end{table}

\subsection{Running Time and Costs}

The cost and time efficiency of the employed models are critical to the feasibility of our automated approach. Tables \ref{tab:time} and \ref{tab:cost} respectively present the average time required and average cost incurred in annotating mathematical identifiers across GPT versions. The pricing is listed on the OpenAI website\footnote{\url{https://openai.com/pricing}}. While GPT-3.5 emerged as the fastest and most time-efficient model, the GPT-4 version manifested as the most costly to operate.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrr}
    \hline
    Model & Dictionary Generation Time & Annotation Time & Total Time \\
    \hline
    gpt (average) & 03:49 & 03:25 & 07:14 \\
    gpt-3.5-turbo & 02:00 & 02:47 & 04:48 \\
    gpt-3.5-turbo-16k & 08:07 & 02:45 & 10:52 \\
    gpt-4 & 01:19 & 04:43 & 06:03 \\
    \hline
  \end{tabular}
  \caption[Average time taken]{Average time taken by each model (mm:ss)}
  \label{tab:time}
\end{table}

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Cost & Cost / 1M Tokens \\
    \hline
    gpt (average) & 1.80 & 9.415 \\
    gpt-3.5-turbo & 0.30 & 1.525 \\
    gpt-3.5-turbo-16k & 0.52 & 2.299 \\
    gpt-4 & 4.59 & 30.449 \\
    \hline
  \end{tabular}
  \caption[Cost Analysis]{Average cost of annotation by each model in USD}
  \label{tab:cost}
\end{table}


\section{Open Source LLMs}
Upon successfully leveraging GPT to automate formula grounding, we proceeded to automate the process using Open Source Models. We applied the same metrics to see how they compete against GPT. The results were again impressive. Due to open-source LLMs' relatively slow speed and high running costs, we selected a subset of 7 papers out of the original 40 by \citet{asakura2022building}. We carefully chose the papers to cover a range of attributes, including high/low CoNLL scores, high/low semantic accuracy, and varied lengths.

\subsection{CoNLL Score}
We applied the same metrics of CoNLL score to the seven selected papers annotated by Open Source LLMs. We compared them against those annotated by GPT, with the human-annotated versions serving as the reference/ground truth. The values can be seen in Table \ref{tab:open-source-conll-score}. Among the open-source models, StableBeluga2 matched the performance of GPT models, indicating its commendable competence in creating high-quality coreference clusters.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    vicuna-33b & 72.44 & 77.45 \\
    gpt (average) & 88.75 & 87.55 \\
    gpt-3.5-turbo & 88.21 & 86.22 \\
    gpt-3.5-turbo-16k & 90.12 & 89.40 \\
    gpt-4 & 87.92 & 87.03 \\
    StableBeluga2 & 84.55 & 81.63 \\
    \hline
  \end{tabular}
  \caption[CoNLL Scores]{Weighted average of CoNLL Scores.}
  \label{tab:open-source-conll-score}
\end{table}

\subsection{Coverage of Annotation}
Open Source models also demonstrated impressive performance in their ability to annotate the papers. The values can be seen in Table \ref{tab:open-coverage}. StableBeluga2 had near-identical performance to the GPT Models.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lr}
    \hline
    Model & Coverage \\
    \hline
    vicuna-33b & 66.18\% \\
    gpt (average) & 91.97\% \\
    gpt-3.5-turbo & 88.93\% \\
    gpt-3.5-turbo-16k & 90.62\% \\
    gpt-4 & 96.35\% \\
    StableBeluga2 & 93.17\% \\
    \hline
  \end{tabular}
  \caption[Total coverage]{coverage of the annotation of the papers achieved by GPT}
  \label{tab:open-coverage}
\end{table}

\subsection{Semantic Accuracy}
For semantic accuracy, we excluded one paper (ArXiV ID: 2107.10832 \citep{singleton2021logic}) due to its length. It was not feasible to manually evaluate it semantically due to its extensive length. We weighted the results against the coverage again to represent the total. As detailed in Table \ref{tab:open-semantic-accuracy}, the semblance between the performance of StableBeluga2 and GPT models reinforces this open-source model's potential in accurately understanding and reflecting the context of scientific papers.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    vicuna-33b & 61.58\% & 40.75\% \\
    gpt (average) & 88.19\% & 81.11\% \\
    gpt-3.5-turbo & 84.69\% & 75.31\% \\
    gpt-3.5-turbo-16k & 84.17\% & 76.27\% \\
    gpt-4 & 95.70\% & 92.21\% \\
    StableBeluga2 & 90.91\% & 84.70\% \\
    \hline
  \end{tabular}
  \caption[Semantic Accuracy]{Weighted average of semantic accuracy}
  \label{tab:open-semantic-accuracy}
\end{table}

\subsection{Variance of Data}
It was impossible to calculate the data variance due to the high costs of running the Open Source Models and budget constraints. However, preliminary testing during the model selection phase indicated that Open Source Models were considerably more stable than GPT.

\subsection{Running Time and Costs}
Given the distinctive operational requirements of open-source models, the computation of time and cost efficiencies differ from those of GPT models. Unlike GPT models, the cost for open-source models revolves around GPU runtime on the servers of \href{https://runpod.io}{runpod.io} and not token usage. Table \ref{tab:open-time} and \ref{tab:open-cost} represent these essential aspects. The pricing of GPT is listed on the OpenAI website\footnote{\url{https://openai.com/pricing}}. The cost of running vicuna-33b was 0.69USD/h, and StableBeluga2 was 1.84USD/h.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrr}
    \hline
    Model & Dictionary Generation Time & Annotation Time & Total Time \\
    \hline
    vicuna-33b & 04:16 & 07:16 & 12:33 \\
    gpt (average) & 01:54 & 01:53 & 03:48 \\
    gpt-3.5-turbo & 01:04 & 01:21 & 02:25 \\
    gpt-3.5-turbo-16k & 04:02 & 02:18 & 06:21 \\
    gpt-4 & 00:38 & 02:01 & 02:39 \\
    StableBeluga2 & 05:17 & 09:43 & 20:20 \\
    \hline
  \end{tabular}
  \caption[Average time taken]{Average time taken by each model (mm:ss)}
  \label{tab:open-time}
\end{table}

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Cost \\
    \hline
    vicuna-33b & 0.14 \\
    gpt (average) & 1.80 \\
    gpt-3.5-turbo & 0.15 \\
    gpt-3.5-turbo-16k & 0.22 \\
    gpt-4 & 2.06 \\
    vicuna-33b & 0.62 \\
    \hline
  \end{tabular}
  \caption[Cost Analysis]{Average cost of annotation by each model in USD}
  \label{tab:open-cost}
\end{table}
