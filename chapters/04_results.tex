\chapter{Results}\label{chapter:results}

The outcomes of the annotation process using GPT and LLMs were compelling. GPT annotations showed quality and precision in explaining the identifiers. This chapter presents the results, including CoNLL scores, semantic accuracy scores, annotation costs, annotation duration, the extent of paper annotation by the LLMs, and the variance in scores due to the stochastic nature of LLMs. All the results are available at \href{https://docs.google.com/spreadsheets/d/1v0t9q5V2j4phjZxXQFIH3b06vl8UBNrFDZlz6ajPAac/edit}{https://docs.google.com/spreadsheets/d/1v0t9q5V2j4phjZxXQFIH3b06vl8UBNrFDZlz} \\ \href{https://docs.google.com/spreadsheets/d/1v0t9q5V2j4phjZxXQFIH3b06vl8UBNrFDZlz6ajPAac/edit}{6ajPAac/edit}.

\section{GPT}
We first scrutinised the annotations generated by GPT. For this analysis, we used all 40 papers from \cite{asakura2022building} as the ground truth and then employed GPT to generate dictionaries and annotations for all these papers.

\subsection{CoNLL Score}
The CoNLL score is a metric used to quantify the quality of coreference clusters. We obtained the weighted average of the CoNLL score across all the papers, with the weighting factored against the total number of annotations in the paper. This means that the calculated average score for every paper weighs the same, regardless of the number of annotations in that paper. Higher scores indicate better performance. As shown in Table \ref{tab:conll-score}, GPT-4 outperformed the other models, while GPT-3.5 exhibited the lowest score.

%% Rune: I suspect one digit of significance after the comma would be more than enough for all the percentages in all the tables below.
\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    gpt (average) & 79.31 & 76.89 \\
    gpt-3.5-turbo & 78.51 & 75.93 \\
    gpt-3.5-turbo-16k & 79.28 & 76.67 \\
    \textbf{gpt-4} & \textbf{80.15} & \textbf{78.08}\\
    \hline
  \end{tabular}
  \caption[CoNLL Scores]{Weighted average of CoNLL Scores.}
  \label{tab:conll-score}
\end{table}


\subsection{Coverage of Annotation}
Another critical aspect of this research was to examine how much of the paper the LLMs could successfully annotate. We present this coverage in Table \ref{tab:anno-percentage}. GPT-4 demonstrated the highest coverage, while GPT-3.5 had the least.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lr}
    \hline
    Model & Coverage \\
    \hline
    gpt (average) & 90.57\% \\
    gpt-3.5-turbo & 90.64\% \\
    gpt-3.5-turbo-16k & 88.21\% \\
    \textbf{gpt-4} & \textbf{92.87}\% \\
    \hline
  \end{tabular}
  \caption[Total coverage]{coverage of the annotation of the papers achieved by GPT}
  \label{tab:anno-percentage}
\end{table}

\subsection{Semantic Accuracy}
Semantic accuracy provides a measure of the correctness of the annotations. We weighted it against the coverage to represent the total. Because of the extensive difficulty of manually reviewing semantic accuracy, we evaluated six carefully picked papers representing various low/high CoNLL scores and lengths. As shown in Table \ref{tab:semantic-accuracy}, GPT-4 again outperformed all the models by a significant margin. There was one instance where GPT-4 achieved an astonishing 100\% semantic accuracy (GPT-3.5: 96.15\%).
%% How many instances are in those papers?
%% What are some comparable numbers reported by older/non-GPT studies?

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    gpt (average) & 88.19\% & 79.87\% \\
    gpt-3.5-turbo & 84.69\% & 76.76\% \\
    gpt-3.5-turbo-16k & 84.17\% & 74.25\% \\
    \textbf{gpt-4} & \textbf{95.70}\% & \textbf{88.88}\% \\
    \hline
  \end{tabular}
  \caption[Semantic Accuracy]{Weighted average of semantic accuracy}
  \label{tab:semantic-accuracy}
\end{table}

\subsection{Variance of Data}
Given the inherent stochastic nature of these models, some variation can be expected across multiple iterations. To account for this, we conducted multiple runs of the experiment on the reference paper ArXiV ID: 2107.10832\footnote{\url{https://arxiv.org/pdf/2107.10832.pdf}} \citep{singleton2021logic} and tabulated the resultant variance. These outcomes are displayed in Table \ref{tab:variance}.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrrrr}
    \hline
    Model & Best & Worst & Mean & Median & Std. Deviation \\
    \hline
    gpt (average) & 86.75 & 83.21 & 84.58 & 84.18 & 1.55 \\
    gpt-3.5-turbo & 82.83 & 80.00 & 81.28 & 81.15 & \textbf{1.17} \\
    gpt-3.5-turbo-16k & 88.38 & 83.47 & 85.29 & 84.65 & 2.16 \\
    \textbf{gpt-4} & \textbf{89.05} & \textbf{86.16} & \textbf{87.18} & \textbf{86.75} & 1.32 \\
    \hline
  \end{tabular}
  \caption[Statistics of variance]{Different CoNLL scores across four different runs}
  \label{tab:variance}
\end{table}

\subsection{Running Time and Costs}

The cost and time efficiency of the employed models are critical to the feasibility of our automated approach. Tables \ref{tab:time} and \ref{tab:cost} respectively present the average time required and average cost incurred in annotating mathematical identifiers across GPT versions. The pricing is listed on the OpenAI website\footnote{\url{https://openai.com/pricing}}. While GPT-3.5 emerged as the fastest and most time-efficient model, the GPT-4 version manifested as the most costly. Table~\ref{tab:relative-cost} describes the relative cost per concept.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrr}
    \hline
    Model & Dictionary Generation Time & Annotation Time & Total Time \\
    \hline
    gpt (average) & 03:49 & 03:25 & 07:14 \\
    gpt-3.5-turbo & 02:00 & 02:47 & \textbf{04:48} \\
    gpt-3.5-turbo-16k & 08:07 & \textbf{02:45} & 10:52 \\
    gpt-4 & \textbf{01:19} & 04:43 & 06:03 \\
    \hline
  \end{tabular}
  \caption[Average time taken]{Average time taken by each model (mm:ss)}
  \label{tab:time}
\end{table}

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Cost & Cost / 1M Input Tokens \\
    \hline
    gpt (average) & 1.80 & 9.415 \\
    \textbf{gpt-3.5-turbo} & \textbf{0.30} & \textbf{1.525} \\
    gpt-3.5-turbo-16k & 0.52 & 2.299 \\
    gpt-4 & 4.59 & 30.449 \\
    \hline
  \end{tabular}
  \caption[Cost Analysis]{Average cost of automation by each model in USD}
  \label{tab:cost}
\end{table}

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Cost / 1000 Annotations (USD) & Time / Annotation (seconds) \\
    \hline
    gpt (average) & 2.49 & 0.73\\
    \textbf{gpt-3.5-turbo} & \textbf{0.40} & \textbf{0.46} \\
    gpt-3.5-turbo-16k & 0.88 & 1.25 \\
    gpt-4 & 6.18 & 0.47 \\
    \hline
  \end{tabular}
  \caption[Cost Analysis]{Relative cost and time taken}
  \label{tab:relative-cost}
\end{table}


\section{Open Source LLMs}
Upon successfully using GPT to achieve formula grounding, we proceeded to do the same using Open Source Models instead. We applied the same metrics to see how they compete. The results were again impressive. Due to open-source LLMs' relatively slow speed (i.e. high run-time costs), we selected a subset of 7 of the original 40 papers by \citet{asakura2022building}. We carefully chose the papers to cover a range of attributes, including high/low CoNLL scores, high/low semantic accuracy, and short/long papers.

\subsection{CoNLL Score}
We applied the same metrics of CoNLL score to the seven selected papers annotated by Open Source LLMs. We compared them against those annotated by GPT, with the human-annotated versions serving as the reference/ground truth. Table \ref{tab:open-source-conll-score} shows the respective values. Among the open-source models, StableBeluga2 matched the performance of GPT models, indicating its ability to create high-quality coreference clusters. Vicuna-33b failed to annotate one paper entirely, so it has a score of 0 for that paper.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    vicuna-33b & 72.44 & 77.45 \\
    gpt (average) & 88.75 & 87.55 \\
    gpt-3.5-turbo & 88.21 & 86.22 \\
    \textbf{gpt-3.5-turbo-16k} & \textbf{90.12} & \textbf{89.40} \\
    gpt-4 & 87.92 & 87.03 \\
    StableBeluga2 & 84.55 & 81.63 \\
    \hline
  \end{tabular}
  \caption[CoNLL Scores]{Weighted average of CoNLL Scores.}
  \label{tab:open-source-conll-score}
\end{table}


\subsection{Coverage of Annotation}
Open Source models also demonstrated impressive performance in their ability to annotate the papers. The values can be seen in Table \ref{tab:open-coverage}. StableBeluga2 had near-identical performance to the GPT Models. Vicuna-33b failed to annotate one paper entirely, so it has a score of 0 for that paper.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lr}
    \hline
    Model & Coverage \\
    \hline
    vicuna-33b & 66.18\% \\
    gpt (average) & 91.97\% \\
    gpt-3.5-turbo & 88.93\% \\
    gpt-3.5-turbo-16k & 90.62\% \\
    \textbf{gpt-4} & \textbf{96.35}\% \\
    StableBeluga2 & 93.17\% \\
    \hline
  \end{tabular}
  \caption[Total coverage]{coverage of the annotation of the papers achieved by GPT}
  \label{tab:open-coverage}
\end{table}

\subsection{Semantic Accuracy}
For semantic accuracy, we excluded the paper\footnote{\url{https://arxiv.org/pdf/2107.10832.pdf}}~\citep{singleton2021logic} due to its length. It was not feasible to manually evaluate it semantically because it was too long. Again, we weighted the results against the coverage to represent the total. As shown in Table \ref{tab:open-semantic-accuracy}, the resemblance between the performance of StableBeluga2 and the GPT models indicates that open-source models also have the potential to accurately "understand" (reflecting the correct context of) scientific papers.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrr}
    \hline
    Model & Score & Weighted Score \\
    \hline
    vicuna-33b & 61.58\% & 40.75\% \\
    gpt (average) & 88.19\% & 81.11\% \\
    gpt-3.5-turbo & 84.69\% & 75.31\% \\
    gpt-3.5-turbo-16k & 84.17\% & 76.27\% \\
    \textbf{gpt-4} & \textbf{95.70}\% & \textbf{92.21}\% \\
    StableBeluga2 & 90.91\% & 84.70\% \\
    \hline
  \end{tabular}
  \caption[Semantic Accuracy]{Weighted average of semantic accuracy}
  \label{tab:open-semantic-accuracy}
\end{table}

\subsection{Variance of Data}
It was impossible to calculate the data variance due to the high costs of running the Open Source Models and our budget constraints. However, preliminary testing during the model selection phase indicated that Open Source Models produce considerably more stable results than the GPT models.

\subsection{Running Time and Costs}
Given the distinctive operational requirements of open-source models, the computation of time and cost efficiencies differ from those of GPT models. Unlike GPT models, the cost for open-source models revolves around GPU run-time on the servers of \href{https://runpod.io}{runpod.io} and not the token usage. Tables \ref{tab:open-time} and \ref{tab:open-cost} represent both the time and actual cost aspects. The pricing of GPT is listed on the OpenAI website\footnote{\url{https://openai.com/pricing}}. The cost of running vicuna-33b was 0.69USD/h, and StableBeluga2 was 1.84USD/h.

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrr}
    \hline
    Model & Dictionary Generation Time & Annotation Time & Total Time \\
    \hline
    vicuna-33b & 04:16 & 07:16 & 12:33 \\
    gpt (average) & 01:54 & 01:53 & 03:48 \\
    gpt-3.5-turbo & 01:04 & \textbf{01:21} & \textbf{02:25} \\
    gpt-3.5-turbo-16k & 04:02 & 02:18 & 06:21 \\
    gpt-4 & \textbf{00:38} & 02:01 & 02:39 \\
    StableBeluga2 & 05:17 & 09:43 & 20:20 \\
    \hline
  \end{tabular}
  \caption[Average time taken]{Average time taken by each model (mm:ss)}
  \label{tab:open-time}
\end{table}

\begin{table}[htpb]
  \centering
  \begin{tabular}{lrrr}
    \hline
    Model & Cost & Cost / 1000 Annotations (USD) & Time / Annotation (sec) \\
    \hline
    vicuna-33b & \textbf{0.14} & 0.51 & 2.73 \\
    gpt (average) & 1.80 & 2.70 & 1.06 \\
    gpt-3.5-turbo & \textbf{0.15} & \textbf{0.43} & 0.60 \\
    gpt-3.5-turbo-16k & 0.22 & 1.01 & 2.10 \\
    gpt-4 & 2.06 & 6.67 & \textbf{0.48} \\
    StableBeluga2 & 0.62 & 2.77 & 5.48 \\
    \hline
  \end{tabular}
  \caption[Cost Analysis]{Average cost of annotation by each model}
  \label{tab:open-cost}
\end{table}
