\chapter{Methodology}\label{chapter:methods}

This chapter describes the methodology employed in this research, encompassing three core stages: 1) pre-processing of mathematical identifiers, 2) dictionary construction, and 3) association of individual IDs to description (annotation) instances. The process uses \LaTeX ML utilities and explores the capabilities of \ac{LLMs} to automate the annotation of mathematical identifiers. Here, we present the transitions from source materials to machine-readable formats and from dictionary formulation to final annotation. The entire code is available at \url{https://github.com/babyygemperor/grounding-of-formulae}.

Before delving into the specifics, it is crucial to understand the structure of \\ MioGatto\footnote{\url{https://github.com/wtsnjp/MioGatto/}} \citep{asakura2021miogatto}, the tool we aim to automate. MioGatto contains three core files: 
\begin{enumerate}
    \item \texttt{source.html}—a pre-processed HTML file suitable for web rendering of the given paper, illustrated in Figure \ref{fig:miogatto-sources}
    \item \texttt{mcdict.json}—a JSON file containing a list of dictionary of possible descriptions for each identifier (Figure \ref{fig:miogatto-data}a), 
    \item \texttt{anno.json}—another JSON file that holds each identifier's index of the chosen description (Figure \ref{fig:miogatto-data}b).
\end{enumerate}

The generation of these three files corresponds to the three first sections in this chapter. 
Section \ref{sec:pre-processing} focuses on creating \texttt{source.html}. 
Section \ref{sec:dic-generation} covers the generation of \texttt{mcdict.json}, and Section \ref{sec:annotation} addresses \texttt{anno.json}. The three core files are interlinked by IDs generated during pre-processing. The structure of the JSON files is depicted in Figure \ref{fig:miogatto-data}.

%% Maybe this figure should also show the "web rendering" described above for increased readability.
\begin{figure}[htpb]
  \centering
    \begin{minipage}{1\textwidth}
      \lstinputlisting[language=html]{sources/source.html}
    \end{minipage}
  \caption[LaTeXML Pre-processing]{\LaTeX \space converted to HTML Format of "A Logic of Expertise" \citep{singleton2021logic}}\label{fig:miogatto-sources}
\end{figure}

\begin{figure}[htpb]
  \centering
  \subfloat[mcdict.json]{
    \begin{minipage}{1\textwidth}
      \lstinputlisting[language=python]{sources/mcdict.json}
    \end{minipage}
  }
  \quad 
  \subfloat[anno.json]{
    \begin{minipage}{1\textwidth}
      \lstinputlisting[language=python]{sources/anno.json}
    \end{minipage}
  }
  \caption[LaTeXML Preprocessing]{Dictionary and Annotation Files of MioGatto}\label{fig:miogatto-data}
\end{figure}

\section{Pre-processing}\label{sec:pre-processing}

Initially, we considered parsing the \LaTeX \space code directly via GPT-based LLMs, owing to their frequent training on \LaTeX \space documents. \LaTeX \space has a more decent semantic layer to mathematical identifiers than ASCII Math and offers efficiency in token usage (see Definition~\ref{def:token}) and complexity compared to MathML. To illustrate the effectiveness of different encodings, Table \ref{fig:ascii-math} displays various representations of identical formulae, comparing \LaTeX, ASCII Math, and MathML (XML) regarding readability and token count.

%% THIS FIGURE WAS TOO FAR DOWN FROM IT FIRST MENTION on line 75
%% It also does not correspond well with Figure 3.2a, as stated in the text on that line {fig:miogatto-data}a
%% MAYBE 3.2 a and b can be moved to an appendix? Or say that they use .json Format?

\begin{figure}[htpb]
  \centering
  \begin{lstlisting}[language=python]
{    
    "$\\equiv$": "Logical equivalence operator",
    "$\\phi$": "A formula in the language $\\cL$",
    "$\\cL$": "Language of expertise and soundness",
    "$\\prop$": "Countable set of propositional variables",
    "$\\univ$": "Universal modality",
    "$\\orr$": "Disjunction operator",
    "$\\subseteq$": "Subset or equal to",
    "$\\in$": "Element of a set",
    "$\\cap$": "Intersection of two sets",
    "$\\subseteq$": "Subset or equal to",
    "$\\emptyset$": "Empty set",
    "$\\sat$": "Satisfaction relation",
    "$\\neg$": "Negation operator",
    "$\\and$": "Conjunction operator",
    "$\\|\phi\\|_M$": "Set of states where $\\phi$ is true in model $M$",
    "$\\forall$": "Universal quantifier"
}
  \end{lstlisting}
  \caption[GPT Dictionary from LaTeX]{Dictionary generated by GPT-3.5 using \LaTeX \space as Source}\label{fig:latex-dict}
\end{figure}

While \LaTeX \space yielded a high-quality dictionary in our initial experiments, a challenge emerged in mapping the keys of the generated \LaTeX \space dictionary to their rendered instances in the final annotation. This necessitated an array of complex heuristics to convert the dictionary generated using \LaTeX \space as source (Figure \ref{fig:latex-dict}) to a MioGatto dictionary (Figure \ref{fig:miogatto-data}a). Converting complex \LaTeX \space equations to ASCII Math would have alleviated the heuristics issue, but it was deemed infeasible due to ASCII Math's limitations of expressing rich content. This also led us to rule out directly ingesting XML because of the prohibitive token count and because \ac{LLMs} lacked training on XML, leading them to generate nonsensical outputs. Consequently, we opted for a pre-processing step to retrieve the identifiers in a format that removed the need for intricate heuristics.


\LaTeX ML\footnote{\url{https://math.nist.gov/~BMiller/LaTeXML/}} \citep{ginev2011latexml} served as the tool of choice for this pre-processing step. The rationale behind this conversion is clear: HTML stands as the source view for further processing with MioGatto and formula grounding. Quite conveniently, \LaTeX ML automatically identifies mathematical symbols and embeds them in an \lstinline{<mi>} tag, making the output machine-readable. We subsequently transform the HTML into our variant of ASCII Math with the following command.

\begin{lstlisting}[language=bash]
latexmlc --preload=[nobibtex,ids,mathlexemes,localrawstyles]latexml.sty
     --format=html5 --pmml --cmml --mathtex --nodefaultresources 
     --dest=<output HTML file> <input TeX file>
\end{lstlisting}

While this format conveys less information than \LaTeX, the robust capabilities of LLMs compensate for this limitation, yielding comparable results. After the HTML generation, MioGatto's pre-processing tool runs to create a template for the dictionary and annotations
  \begin{lstlisting}[language=bash]
    python -m tools.preprocess <HTML file>
  \end{lstlisting}



\begin{table}[htpb]
  \centering
  \subfloat[Quadratic Equation]{%
    \begin{tabular}{llr}
      \hline
      Encoding & Formula & Tokens\\
      \hline
      \LaTeX & \( x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a} \) & 24 \\
      ASCII Math & \texttt{x = (-b +/- sqrt\{b\^{}2 - 4ac\})/(2a)} & 23 \\
      XML & Shown in Appendix \ref{app:xml_quadratic} & 387 \\
      \hline
    \end{tabular}
  }
  \quad 
  \subfloat[Ampere's Circuit Law]{%
    \begin{tabular}{llr}
      \hline
      Encoding & \begin{tabular}[t]{@{}l@{}}Formula\end{tabular} & Tokens\\
      \hline
      \LaTeX & \( \oint_C \vec{B}\circ \mathrm{d}\vec{l} = \mu_0 \left( I_{\text{enc}} + \varepsilon_0 \frac{\mathrm{d}}{\mathrm{d} t} \int_S \vec{E} \circ \hat{n}\; \mathrm{d} a \right) \) & 84 \\
      ASCII Math & \begin{tabular}[t]{@{}l@{}}\texttt{oint\_C (B . dl)=mu\_0*(I\_\{enc\} +} \\ \texttt{eps\_0 * d/dt * int\_S (E . n\_\{hat\}) da)}\end{tabular} & 42 \\
      XML & Shown in Appendix \ref{app:xml_ampere} & 929 \\
      \hline
    \end{tabular}
  }
  \caption[Token Usages]{Token usages of different types of encoding}
  \label{fig:ascii-math}
\end{table}



\section{Dictionary Generation}\label{sec:dic-generation}

Transforming raw HTML data into a comprehensive dictionary required several different tactics. Initial attempts included \ac{POS} tagging. It seemed convenient as, in most academic texts, identifier definitions are placed either before or after the identifier's first introduction.

For instance, within the excerpt from the paper "A Logic of Expertise" by  \citet{singleton2021logic} as shown in Figure \ref{fig:POS_right}, "$\mathsf{Prop}$" is introduced and immediately followed by its definition "\textit{A countable set of propositional variables}".

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \includegraphics[width=14cm]{images/POS_right.png}
  \end{tabular}
  \caption[POS Tagging Right]{Screenshot of MioGatto showing the paper "A Logic of Expertise" with description to the right of the identifier Prop}\label{fig:POS_right}
\end{figure}

Similarly in Figure \ref{fig:POS_left}, `$\mathcal{L}$` is described as "$language$" immediately prior to its mention.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \includegraphics[width=14cm]{images/POS_left.png}
  \end{tabular}
  \caption[POS Tagging Left]{Screenshot of MioGatto showing the paper "A Logic of Expertise" with description to the left of the identifier L}\label{fig:POS_left}
\end{figure}

While seemingly effective for cases like these, this pattern only works for some cases. In several instances across academic works, the definition does not directly proceed or follow the identifier, making POS tagging less fruitful. Take, for example, the snippet shown in Figure~\ref{fig:POS_failed} from the same paper mentioned earlier. Here, our mathematical understanding identifies "$X$" as a set, which is not readily inferred by POS tagging or formal grammar alone.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \includegraphics[width=14cm]{images/POS_failed.png}
  \end{tabular}
  \caption[POS Tagging Left]{Screenshot of MioGatto showing the paper "A Logic of Expertise" without a possible POS Tagging}\label{fig:POS_failed}
\end{figure}

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=python]
    {
        "P": [
            "Expertise set"
            "Set of properties P1, P2, P3",
            "Mapping to RP"
            "Set of all unions of equivalence classes"
        ],
        "M": [
            "Expertise model",
            "Model"
            "S5 model",
            "Non-augmented model obtained from N' by dropping the RA'
             component"
        ], 
        "p": "Proposition",
        "r": [
            "Economic recovery proposition",
            "Report proposition"
        ],
        "s", "Source"
        "L": "Language of expertise and soundness"
    }
  \end{lstlisting}
  \end{tabular}
  \caption[Response from Main Prompt for Dictionary Generation]{Response from GPT using the Prompt for Dictionary Generation}\label{fig:dic_response}
\end{figure}

This complexity led to exploring the capabilities of Large Language Models (LLMs) like GPT-3.5 from OpenAI. While LLMs were generally designed as chat models and not specifically for mathematical text or dictionary generation, preliminary trials proved promising. Upon feeding a paragraph of text to the model through OpenAI's API, it returned a well-formatted dictionary (Figure \ref{fig:dic_response}) that was highly usable for annotation. However, an obstacle emerged that the model's context window is limited. Most of the papers we tested were (at least) 20-40 thousand tokens long, and the context window of the LLMs is not big enough to accommodate this (see Table~\ref{tab:token_counts}).
%% Better text? "20-40 thousand tokens" or "20k-40k tokens"
%% Rune: Define Tokens as roughly equivalent to the Number of Words. 

\begin{table}[h]
    \centering
    \begin{tabular}{lrr}
        \hline
        Model & Context Window (Tokens) & Chunks Size (Tokens)\\
        \hline
        GPT-3.5-turbo & 4096 & 1750 \\
        GPT-3.5-16k-turbo & 16384 & 2000 \\
        GPT-4 & 8192 & 4000 \\
        \hline
    \end{tabular}
    \caption{Token counts for different models}
    \label{tab:token_counts}
\end{table}

To deal with the overflow issue, each paper was divided into smaller chunks, approximately half the size of the respective model's context window. The context window varies for different \ac{NLP} models as well, and the chosen sizes of chunks are shown in Table \ref{tab:token_counts}. This adjustment accounted for the tokens generated by the API and made allowances for the length of our prompts. The token count was carefully reduced for GPT-3.5-16k-turbo because the model tended to generalise the description of the identifiers when there were many occurrences in larger chunk sizes.

After dividing the paper into smaller overlapping chunks, they are used as inputs to the OpenAI API iteratively to generate a dictionary associated explicitly with each chunk. The chunks were carefully constructed so they did not fragment paragraphs and thus maintained the integrity of any crucial contextual information. Furthermore, to mitigate context loss when transitioning from one chunk to another, the generated dictionary was looped back into the prompt, which ensured the LLM maintained awareness of the other possible definitions of the identifiers. The system prompt (see Definition~\ref{def:prompt}) (Figure~\ref{fig:prompt_dic_system}) was meticulously designed, through experimentation, to provide the best possible results. After the system prompt, an example of the desired dictionary format was presented, as shown in Figure \ref{fig:prompt_dic_example}. Considering this approach involves neither zero-shot learning nor one-shot/few-shot learning, we call it  'half-shot learning'. 

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=python]
    {'role': 'system',
    'content': 'You are a helpful research assistant tasked with converting
    long paragraphs into a Python dictionary. The goal is to identify and 
    classify each individual mathematical symbol, variable, and identifier 
    in the text marked between "<||>". The dictionary should store the 
    identifiers as keys and their corresponding definitions as values 
    in an array format. '}
  \end{lstlisting}
  \end{tabular}
  \caption[System Prompt for Dictionary Generation]{System Prompt for Dictionary Generation}\label{fig:prompt_dic_system}
\end{figure}

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \includegraphics[width=14cm]{images/Prompt_dict_example.png}
  \end{tabular}
  \caption{Half Shot Learning Example Demonstrated in the Prompt for Dictionary Generation}\label{fig:prompt_dic_example}
\end{figure}

The prompt containing the chunks (Figure~\ref{fig:prompt_dic}) is then forwarded to the next stage. The previously generated dictionary is attached as supplementary context to help deal with the token limit context window.
% context - contend - content is somewhat confusing.
However, if the prompt is excessively long, the previous dictionary is omitted, and this scenario is the only occurrence of context loss from one iteration to another. The chunk is submitted, and the response received is parsed and incorporated into the master dictionary. This procedure is repetitively enacted until all the chunks have passed through the LLM and a comprehensive master dictionary is generated. This essential JSON dictionary (Figure \ref{fig:dic_response}) then gets converted into a MioGatto-compatible dictionary (Figure \ref{fig:miogatto-data}a) using basic mapping techniques.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
  \begin{lstlisting}[language=python]
    {'role': 'system',
    'content': 'Given is already a pre-existing dictionary. 
    Your job is to extend this dictionary. Do not remove
    any pre-existing definitions from this dictionary.  \n'
    + dictionary[index] + '. 
    If there is nothing to mention, reply with an empty dictionary'},
    {'role': 'user', 'content': 'Generate a Python dictionary for the 
    the following text:' 
    + chunk + 
    'Only consider the mathematical identifiers inside "<||>"
    for the dictionary. 
    Do not consider any other identifier other than those marked.
    Consider all the identifiers individually, and do not skip
    any identifier, mention all the identifiers inside 
    "<||>" in your dictionary. Do not include the angle 
    brackets in your dictionary."}
  \end{lstlisting}
  \end{tabular}
  \caption[Main Prompt for Dictionary Generation]{Main Prompt for Dictionary Generation} \label{fig:prompt_dic}
\end{figure}


\section{Association of ID to Description Occurrence}\label{sec:annotation}

To annotate instances of identifiers with appropriate descriptions, we again employed \ac{LLMs} to select suitable annotations. We designed specific prompts to optimise the LLM's performance. Figure \ref{fig:prompt_anno_system} shows the system prompt used for this purpose. We set the temperature to 0 to ensure consistency and to avoid hallucination for the annotations.
%% I am not sure 0 ensures no hallucination! Sometimes, the most probable next token is a hallucination. (If the paper looks more like a previously trained-on paper than the new exclusive thing it is, the intersection might be a "non-existing" hallucination, no?

To enable annotation by the LLM, we slightly modified the generated dictionary to include additional information. The LLM receives the identifier to annotate, a dictionary of potential annotations (including possible affixes), and the context (Figure \ref{fig:prompt_anno_main}b). The context consists of approximately 75 tokens to the left and 25 to the right of the identifier. For GPT-4, we used 40 tokens to the left and 10 to the right to reduce computational costs without noticeably sacrificing quality.

Based on this information, the LLM selects the most suitable annotation. The whole context for the annotation is presented in Figure \ref{fig:prompt_anno_main}. This process is repeated for all identifiers. If an identifier has already been annotated, its description serves as context for subsequent identifiers within the same context window (See Figure \ref{fig:prompt_anno_main} where in context the definition of E is known due to the previous iteration where that identifier as annotated). This process is advantageous for long paragraphs of identifiers but can also lead to cascading errors if an identifier is misannotated. Special consideration is given to identifiers whose affixes match those in the dictionary. If an identifier has only one possible description, it is automatically selected, reducing the computational load on the LLM.

\begin{figure}[htpb]
  \centering
  \begin{lstlisting}[language=python]
    {
        "role": "system",
        "content": "You are a professional annotator API. Your job is to 
        select a fitting annotation from a dictionary for a mathematical
        identifier."
    }
  \end{lstlisting}
  \caption[System Prompt for Annotation]{System Prompt for Annotation}\label{fig:prompt_anno_system}
\end{figure}

\begin{figure}[htpb]
  \centering
  \subfloat[User Prompt]{
    \begin{minipage}{1\textwidth}
      \lstinputlisting[language=python]{sources/user_prompt.py}
    \end{minipage}
  }
  \quad 
  \subfloat[User Prompt's Variables]{
    \begin{minipage}{1\textwidth}
      \lstinputlisting[language=python]{sources/user_prompt_var.py}
    \end{minipage}
  }
  \caption[User Prompt for Annotation]{Main Prompt for Annotation with context}\label{fig:prompt_anno_main}
\end{figure}

\section{Utilising Open Source LLMs}

We experimented with Open Source LLMs upon successfully leveraging GPT models for automating formula annotations. It is essential to recognise that OpenAI's models, including GPT-3.5 with 175B parameters and the GPT-4 rumoured to have over 1.8T parameters (almost two trillion), outstrip most open-source variants, which generally max out at around 70B (billion) parameters. However, GPT models are designed for general-purpose tasks, whereas our focus is primarily instructional. We hypothesised that "instruct" models could offer comparable performance.

Initial tests with falcon-40b-instruct\footnote{\url{https://huggingface.co/tiiuae/falcon-40b-instruct}} \citep{falcon40b, refinedweb, xu2023baize}, a high-ranking model on the Hugging Face leaderboard\footnote{\url{https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard}} \citep{jain2022hugging}, were unsuccessful due to its limited context window. Moreover, many LLMs struggled to generate a well-formatted JSON suitable to our needs. After evaluating multiple alternatives, we selected Superhot models \citep{chen2023extending}. These models offer extended context windows compatible with our over 1,000 token prompts. We also chose quantised models for improved efficiency while retaining high performance\footnote{\url{https://medium.com/@developer.yasir.pk/quantized-large-language-model-e80bdcb81a52}}. Specifically, we used \href{https://huggingface.co/TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ}{vicuna-33b}\footnote{\url{https://huggingface.co/TheBloke/Vicuna-33B-1-3-SuperHOT-8K-GPTQ}}~\citep{zheng2023judging}, and \href{https://huggingface.co/TheBloke/StableBeluga2-70B-GPTQ}{StableBeluga2}\footnote{\url{https://huggingface.co/TheBloke/StableBeluga2-70B-GPTQ}}~\citep{StableBelugaModels, touvron2023llama, mukherjee2023orca}, a fine-tuned model of LLaMa and LLaMa-2 respectively. \href{https://huggingface.co/TheBloke/StableBeluga2-70B-GPTQ}{StableBeluga2} was the top-ranked model on the Hugging Face leaderboard at the time of selection.

We reduced the chunk sizes to 750 tokens without noticeably sacrificing quality to adapt to the slower token generation rate and potentially smaller comprehension capabilities. Although the prompts remained identical, we modified their structure from JSON to plain text to suit the transformer models better (see Figure~\ref{fig:open-source-prompt-structure}). 

These open-source LLMs occasionally produced repetitive or incomplete JSONs, necessitating an extension of our dictionary generation approach to handle such irregularities. The transformer settings were:\\
\texttt{temperature=0.5, max\_new\_tokens=512,} \texttt{ repetition\_penalty=1.05}.\\
All other settings remained consistent with previous configurations.

\begin{figure}[htpb]
  \centering
  \begin{lstlisting}
### SYSTEM: <system message>
### USER: <example user message>
### ASSISTANT: <example assistant output>
### USER: <actual user message>
### ASSISTANT:
  \end{lstlisting}
  \caption[System Prompt for Annotation]{System Prompt of Open Source LLMs for Formula Grounding}\label{fig:open-source-prompt-structure}
\end{figure}

\section{Setup for the Experiments}

Experiments were conducted on 40 academic papers using OpenAI's LLMs and our selected open-source models. The papers were chosen by \citet{asakura2022building}. Due to the stochastic nature of LLMs, on rare occasions, multiple runs had to be performed to obtain reliable results. The open-source models were computationally intensive despite being quantised, requiring up to 80GB of \ac{VRAM}. We used cloud \ac{GPU}s for their affordability and ease of setup. Specifically, experiments involving open-source LLMs were executed on \href{https://runpod.io}{runpod.io} with the following configurations:

\begin{itemize}
    \item Vicuna-33b: 1x NVidia L40 (48GB VRAM), 250GB RAM, 32vCPU at \$0.69/h
    \item StableBeluga2: 1x NVidia A100 SXM (80GB VRAM), 251GB RAM, 16vCPU at \$1.84/h
\end{itemize}

\section{Evaluation Metrics}

To rigorously evaluate the different models' performance, we employ two primary metrics: 1) CoNLL Score \citep{pradhan2012conll}, and 2) Semantic Accuracy.

\subsection{CoNLL Score}
The CoNLL score serves as a standard quantitative measure for evaluating the quality of coreference resolution. We calculated this metric using the human-annotated papers generated by \citet{asakura2022building} as the ground truth using CorefUD Scorer\footnote{\url{https://github.com/ufal/corefud-scorer}}. Traditional CoNLL scoring focuses solely on the quality of the coreference clusters—i.e., how well the model groups referring expressions together. However, it does not account for the semantic accuracy of the annotation behind these coreferent items, leading to potential issues in interpretability. For instance, Figure \ref{fig:coreference} illustrates that correctly identifying "Bob" and "he" as part of the same coreference cluster would result in a high CoNLL score. However, if the annotation erroneously labels them as "Alice," semantic integrity is lost, necessitating an additional metric.

\begin{figure}[htpb]
  \centering
  \begin{minipage}{\textwidth}
    \centering
    \includegraphics[width=14cm]{images/coreference.png}
    \caption[Example for coreference]{Example of Coreference Clustering \citep{asakura2022building}\footnotemark}
    \label{fig:coreference}
  \end{minipage}
\end{figure}
\footnotetext{\url{https://speakerdeck.com/wtsnjp/lrec2022?slide=4}}

\subsection{Semantic Accuracy}

Determining semantic accuracy presents several challenges. As there is no author-provided ground truth for the papers, establishing the "correctness" of an annotation becomes complex. Moreover, the subtleties in possible annotations—illustrated in Figure \ref{fig:semantic-incorrectness}—make automated semantic evaluation difficult. For instance, the identifier $\mathbf{}{M}$ could be annotated as either an \texttt{Expertise Model} or the \texttt{Expertise counterpart of M'}, and traditional NLP similarity metrics like cosine distance are ill-suited for this evaluation, as the cosine distance between two similar annotations can be minimal with drastically different mathematical interpretations. Therefore, we resorted to manual reviews of the annotations, categorising them as "correct" or "incorrect." Given the time-consuming nature of this method, we limited our review to a representative subset of 6 of the 40 papers initially selected.

\begin{figure}[htpb]
  \centering
  \begin{tabular}{c}
    \includegraphics[width=14cm]{images/semantic-incorrectness.png}
  \end{tabular}
  \caption[Semantic Correctness]{Challenges in Determining Semantic Correctness}\label{fig:semantic-incorrectness}
\end{figure}
