\chapter{Related Work}\label{chapter:related_work}

The burgeoning field of \ac{MLP} is increasingly focused on the complexities of understanding, annotating, and disambiguating mathematical text. This chapter offers a critical review of seminal works that have shaped the trajectory of this domain, thereby contextualising the relevance of the present study.

\section{Understanding Mathematical Text and Formulae}

\citet{meadows2022survey}'s research emphasises the importance of informal mathematical text in quantitative reasoning and advocates using transformer models like GPT in formula retrieval. This notion is further extended by the \ac{MLP} project \citep{pagael2014mathematical}, which delves into the semantics of identifiers in mathematical formulae. The project contrasts traditional pattern-matching techniques with a novel approach that employs \ac{POS} tag-based distances to deduce identifier-definition probabilities. \citet{grigore2009towards}'s work complements these studies by exploring the complexities of interpreting symbolic expressions in mathematical narratives, highlighting the value of linguistic context.

\section{Foundational Frameworks and Automation}

The pioneering work by \citet{asakura2020towards} serves as a cornerstone in the field, delineating the importance of grounding mathematical formulae. It delineates the importance of anchoring mathematical formulae. The authors champion the indispensable role of MLP in deciphering STEM manuscripts and introduce MioGatto, a cutting-edge annotation tool. \citet{ding2022gpt}'s investigation complements this and \citet{schubotz2017evaluating}'s study, which collectively underscores the transformative potential of automation in data annotation. While Ding focuses on the role of GPT-3 in reducing annotation overheads, Schubotz sets the stage for automated extraction frameworks for mathematical identifier definitions. These works collectively inform the methodology and insights that have shaped our approach. \citet{alexeeva2020mathalign} extend this discourse by introducing machine reading techniques for extracting mathematical concepts, offering a rule-based strategy that aligns \LaTeX \space representations with textual counterparts.

\section{Role of Large Language Models and Pre-trained Frameworks}

The advent of pre-trained models like "MathBERT" \citep{peng2021mathbert} and the evaluation of GPT-3.5 \citep{he2023annollm} mark significant milestones in the field. MathBERT is fine-tuned for decoding mathematical formulas and emphasises the importance of context. At the same time, the research on GPT-3.5 evaluates its efficacy as a robust annotator, questioning its potential to replace traditional crowdsourced methods. These works collectively highlight the untapped potential of Large Language Models in automating the annotation process.

\section{Conclusion}

The body of work reviewed herein provides a comprehensive overview of the challenges and innovations that define the landscape of mathematical language processing. As the field evolves, these cornerstone studies offer invaluable insights, setting the stage for future advancements in this domain.