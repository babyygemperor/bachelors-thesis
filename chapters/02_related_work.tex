\chapter{Related Work}\label{chapter:related_work}

The field of Mathematical Language Processing (MLP) has been progressively evolving, focusing on the complexities of understanding, annotating, and disambiguating mathematical text. This chapter critically reviews key papers that have significantly influenced this domain, thereby contextualizing our current study.

\section{Understanding Mathematical Text and Formulae}

The early work by \citet{grigore2009towards} laid the groundwork for understanding the complexities of interpreting symbolic expressions in mathematical narratives. They emphasized the importance of linguistic context in deciphering mathematical formulae. Building upon this, the MLP project by \citet{pagael2014mathematical} introduced a novel approach that employs Parts of Speech (POS) tag-based distances to estimate the probabilities of identifier-definition relationships, contrasting this with traditional pattern-matching techniques. Most recently, \citet{meadows2022survey} extended the discourse by advocating the use of transformer models like GPT for formula retrieval and emphasized the role of informal mathematical text in quantitative reasoning.

\section{Foundational Frameworks and Automation}

The pioneering work by \citet{asakura2020towards} is a cornerstone. It highlights the importance of anchoring (or grounding) mathematical formulae. The authors show the vital role of \ac{MLP} in deciphering \ac{STEM} manuscripts, and they introduce MioGatto, a cutting-edge annotation tool. An investigation by \citet{ding2022gpt} complements this, and together with other papers, like \citet{schubotz2017evaluating}, they highlight the potential of automation in data annotation. While Ding focuses on the role of GPT-3 in reducing annotation overheads, Schubotz sets the stage for automated extraction frameworks for mathematical identifier definitions. These works collectively contain the methodology and insights that have shaped our approach. \citet{alexeeva2020mathalign} extend this discourse by introducing machine reading techniques for extracting mathematical concepts, offering a rule-based strategy that aligns \LaTeX \space representations with textual counterparts.

\section{The Role of Large Language Models (LLMs) and Pre-trained Frameworks}

The advent of pre-trained models like "MathBERT" \citep{peng2021mathbert} and the evaluation of GPT-3.5 \citep{he2023annollm} mark significant milestones in the field. MathBERT is fine-tuned for decoding mathematical formulae and emphasises the importance of context. At the same time, the research on GPT-3.5 evaluates its efficacy as a robust annotator, questioning its potential to replace traditional crowdsourced methods. These works collectively highlight the untapped potential of \ac{LLMs} in automating the annotation process.

\section{Summary}

The body of work reviewed here provides a comprehensive overview of the challenges of using \ac{MLP} to annotate mathematical text and innovations that define the landscape of \ac{MLP}. As the field evolves, these cornerstone studies offer invaluable insights, setting the stage for future advancements in this domain.
