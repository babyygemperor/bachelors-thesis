\chapter{Related Work}\label{chapter:related_work}

Mathematical Language Processing (MLP) is a burgeoning area of research, focusing on the complexities of understanding, annotating, and disambiguating mathematical text. This chapter critically reviews seminal papers that have significantly influenced this domain, contextualising our current study.

\section{Understanding Mathematical Text and Formulae}

The early work by \citet{grigore2009towards} is a foundational study in mathematical language processing. Grigore emphasised the importance of linguistic context in deciphering mathematical formulae, primarily focusing on symbolic expressions within mathematical narratives. While this work laid the groundwork, it did not delve into automation or using machine-learning models for annotation.

Building upon this, the MLP project by \citet{pagael2014mathematical} introduced a novel approach employing \ac{POS} tag-based distances to estimate the probabilities of identifier-definition relationships. This approach contrasts with traditional pattern-matching techniques, offering a more nuanced understanding of mathematical identifiers. However, it still relies on manual intervention for accurate results.

Most recently, \citet{meadows2022survey} extended the discourse by advocating using transformer models like GPT for formula retrieval. Meadows emphasised the role of informal mathematical text in quantitative reasoning, opening the door for more automated approaches. Unlike our study, Meadows' research focuses not on annotation automation but on formula retrieval.

\section{Foundational Frameworks and Automation}

The pioneering work by \citet{asakura2020towards} is a cornerstone. Asakura and colleagues introduced MioGatto, a cutting-edge annotation tool that anchors or grounds mathematical formulae. Their work highlights the importance of MLP in deciphering STEM manuscripts, but it does not explore the automation of the annotation process, which is the focal point of our study.

\citet{ding2022gpt} and \citet{schubotz2017evaluating} collectively highlight the potential of automation in data annotation. Ding specifically focuses on the role of GPT-3 in reducing annotation overheads but does not delve into the evaluation metrics or the semantic accuracy of the annotations. Schubotz, on the other hand, sets the stage for automated extraction frameworks for mathematical identifier definitions but does not employ \ac{LLMs} for this purpose.

\citet{alexeeva2020mathalign} extend this discourse by introducing machine reading techniques for extracting mathematical concepts. Their rule-based strategy aligns \LaTeX \space representations with textual counterparts, offering a more structured but less flexible approach than our data-driven methodology.

\section{The Role of Large Language Models (LLMs) and Pre-trained Frameworks}

The advent of pre-trained models like "MathBERT" \citep{peng2021mathbert} and the evaluation of GPT-3.5 \citep{he2023annollm} mark significant milestones. MathBERT is fine-tuned for decoding mathematical formulae and emphasises the importance of context. However, it is not designed for annotating mathematical identifiers, which is the primary focus of our study.

The research on GPT-3.5 evaluates its efficacy as a robust annotator and questions its potential to replace traditional crowdsourced methods. While this work aligns closely with our study, it does not provide a comprehensive evaluation using metrics like CoNLL or semantic accuracy, which are central to our research.

\section{Summary}

The body of work reviewed here offers a comprehensive landscape of the challenges and innovations in MLP. These cornerstone studies provide invaluable insights and reveal gaps our research aims to fill. As the field evolves, our study builds upon these foundational works, introducing automation and comprehensive evaluation metrics to advance the domain of mathematical identifier annotation.
