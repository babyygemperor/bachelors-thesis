\chapter{Related Work}\label{chapter:related_work}

Mathematical Language Processing (MLP) is an evolving domain with increasing attention being accorded to the intricacies of mathematical text understanding, annotation, and disambiguation. This chapter critically reviews prominent works, establishing the trajectory of the domain and elucidating its relevance to the present investigation.

\section{Mathematical Text and Quantitative Reasoning}

Meadows' research \parencite{meadows2022survey} places a spotlight on the crucial role of informal mathematical text in quantitative reasoning. The study advocates for the utility of transformer models, like GPT, in formula retrieval and solving math word problems. It asserts that the marriage of linguistic context with structured mathematical knowledge databases can profoundly improve comprehension.

\section{Deciphering Mathematical Formulae}

Within the ambit of the "Mathematical Language Processing (MLP) project" \parencite{pagael2014mathematical}, a comprehensive analysis of the semantics underlying identifiers in mathematical formulae is undertaken. The paper juxtaposes traditional pattern-matching techniques with a novel MLP approach that employs part-of-speech (POS) tag-based distances to deduce identifier-definition probabilities. While the method illustrated the potential of improving user interactivity via tooltips showcasing probable definitions, its efficacy was curtailed in scenarios marked by the intricate natural language descriptions of mathematical identifiers.

\section{Interpreting Symbolic Expressions}

Grigore's work \parencite{grigore2009towards} delves into the complexity of comprehending symbolic expressions present in mathematical narratives. The paper underscores the merit of harnessing linguistic context to fathom the semantics of these expressions, especially when confronted with symbol overloadingâ€” a limitation that deterred the adaptation of certain pre-existing tools.

\section{Innovations in Data Annotation}

Ding's investigation \parencite{ding2022gpt} champions the role of GPT-3 in revolutionizing data annotation, positing a potential reduction in annotation overheads. The study conceptualizes the process as funnelling the expansive knowledge of GPT-3 into nimble networks apt for production landscapes.

\section{The Advent of Automated Extraction and Annotation Systems}

Schubotz's exploration \parencite{schubotz2017evaluating} around the automated extraction of mathematical identifier definitions sets the stage for innovative annotation frameworks. The methodology and insights from this study have been instrumental in shaping our approach.

\section{Harnessing Machine Reading for Mathematical Concepts}

The treatise by Alexeeva et al. \parencite{alexeeva2020mathalign} underscores the transformative power of machine reading in extracting mathematical concepts. The study introduces a judicious rule-based strategy that seamlessly extracts LaTeX representations of formula identifiers and aligns them with their textual counterparts.

\section{Laying the Groundwork for Mathematical Formulae}

The pioneering work by Asakura et al. \parencite{asakura2020towards} delineates the importance of anchoring mathematical formulae. The authors champion the indispensable role of MLP in deciphering STEM manuscripts and introduce MioGatto, a cutting-edge annotation tool.

\section{Evolving Pre-trained Models for Formula Insight}

The innovative "MathBERT" framework \parencite{peng2021mathbert} emerges as a pre-trained model fine-tuned for decoding mathematical formulas. Its training regimen, which couples formulas with their contextual narratives, attests to the centrality of context in deciphering mathematical formulae. While transformer models like MathBERT are of value, our research necessitated models that could craft comprehensive identifier description dictionaries. Moreover, the absence of BERT variants accommodating expansive context windows, spanning up to 1000 tokens, prompted us to adopt different strategies.

\section{Redefining Annotation with Large Language Models}

His seminal research \parencite{he2023annollm} evaluates the efficacy of GPT-3.5 as a robust annotator. The paper interrogates if GPT-3.5's vast training could usher in a paradigm shift, replacing traditional crowdsourced annotators. Their findings accentuate the promise of deploying GPT for annotation endeavours.

\section{Conclusion}

The scholarship reviewed herein illuminates the myriad challenges and innovations characterising mathematical language processing. As the landscape evolves, these foundational works offer invaluable insights and lessons, setting the stage for the next wave of advancements in the field.